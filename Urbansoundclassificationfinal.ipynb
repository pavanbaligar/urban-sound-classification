{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEnXH5MYBNyM",
        "outputId": "85734f39-945a-4d7b-a9b4-4c3baf36bffb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "'Colab Notebooks'  'document '\t urbansoundclassification\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "!ls \"/content/drive/My Drive\"\n",
        "\n",
        "#directories\n",
        "train_labels_df = pd.read_csv('/content/drive/My Drive/urbansoundclassification/audio datasets2/train.csv')\n",
        "test_labels_df = pd.read_csv('/content/drive/My Drive/urbansoundclassification/audio datasets1/test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import soundfile as sf\n",
        "import librosa\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "import joblib\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Function to extract features from an audio file\n",
        "def extract_features(file_name):\n",
        "    try:\n",
        "        # Load the audio file using soundfile\n",
        "        audio, sample_rate = sf.read(file_name)\n",
        "\n",
        "        # Convert to mono if it's stereo\n",
        "        if len(audio.shape) > 1:\n",
        "            audio = audio.mean(axis=1)\n",
        "\n",
        "        # Extract MFCC features using librosa\n",
        "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
        "\n",
        "        # Return the mean of the MFCCs\n",
        "        return mfccs.mean(axis=1)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_name}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to process audio directory and extract features\n",
        "def process_audio_directory(directory):\n",
        "    features = []\n",
        "    file_names = []\n",
        "\n",
        "    if not os.path.exists(directory):\n",
        "        print(f\"Error: Directory '{directory}' not found.\")\n",
        "        return None\n",
        "\n",
        "    for file_name in os.listdir(directory):\n",
        "        if file_name.endswith('.wav'):\n",
        "            file_path = os.path.join(directory, file_name)\n",
        "            feature_vector = extract_features(file_path)\n",
        "            if feature_vector is not None:\n",
        "                features.append(feature_vector)\n",
        "                file_names.append(file_name)\n",
        "\n",
        "    if len(features) == 0:\n",
        "        print(f\"No .wav files found in '{directory}' or all files caused errors.\")\n",
        "        return None\n",
        "\n",
        "    features_df = pd.DataFrame(features)\n",
        "    features_df['file_name'] = file_names\n",
        "    return features_df\n",
        "\n",
        "# Load labels\n",
        "def load_labels(file_path):\n",
        "    try:\n",
        "        return pd.read_csv(file_path)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File '{file_path}' not found.\")\n",
        "        return None\n",
        "\n",
        "# Function to extract ID from file name\n",
        "def extract_id_from_filename(file_name):\n",
        "    try:\n",
        "        return int(file_name.split('.')[0])\n",
        "    except (ValueError, IndexError):\n",
        "        return None\n",
        "\n",
        "# Optional plotting function\n",
        "def plot_confusion_matrix(conf_matrix, y_labels):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=y_labels, yticklabels=y_labels)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.show()\n",
        "\n",
        "def plot_feature_importance(importances):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(range(len(importances)), importances)\n",
        "    plt.title('Feature Importance')\n",
        "    plt.xlabel('Feature Index')\n",
        "    plt.ylabel('Importance')\n",
        "    plt.show()\n",
        "\n",
        "def plot_predicted_class_distribution(predictions):\n",
        "    predictions['predicted_class'].value_counts().plot(kind='bar', figsize=(10, 6))\n",
        "    plt.title('Predicted Class Distribution on New Test Set')\n",
        "    plt.xlabel('Class')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.show()\n",
        "\n",
        "# Set this flag to True to enable plotting\n",
        "ENABLE_PLOTTING = True\n",
        "\n",
        "# Process train dataset\n",
        "train_audio_directory = '/content/drive/My Drive/urbansoundclassification/audio datasets2/Train'\n",
        "train_features_df = process_audio_directory(train_audio_directory)\n",
        "\n",
        "if train_features_df is not None:\n",
        "    # Load train labels\n",
        "    train_labels_df = load_labels('/content/drive/My Drive/urbansoundclassification/audio datasets2/train.csv')\n",
        "\n",
        "    if train_labels_df is not None:\n",
        "        # Extract IDs from file names and convert them to integers\n",
        "        train_features_df['ID'] = train_features_df['file_name'].apply(extract_id_from_filename)\n",
        "        train_features_df = train_features_df.dropna(subset=['ID'])\n",
        "        train_features_df['ID'] = train_features_df['ID'].astype(int)\n",
        "        train_labels_df['ID'] = train_labels_df['ID'].astype(int)\n",
        "\n",
        "        # Merge features with labels\n",
        "        merged_df = pd.merge(train_features_df, train_labels_df, on='ID')\n",
        "\n",
        "        # Separate features and labels\n",
        "        X = merged_df.drop(columns=['file_name', 'ID', 'Class'])  # Drop non-numeric columns\n",
        "        y = merged_df['Class']\n",
        "\n",
        "        # Split the data into 70% training and 30% testing\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "        # Scale features\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "        # Train RandomForestClassifier model with cross-validation\n",
        "        model = RandomForestClassifier(n_estimators=100)  # Using 100 trees\n",
        "\n",
        "        # Perform 5-fold cross-validation on the training set\n",
        "        cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)\n",
        "\n",
        "        # Print cross-validation results\n",
        "        print(f\"Cross-validation accuracy scores: {cv_scores}\")\n",
        "        print(f\"Average cross-validation accuracy: {cv_scores.mean():.4f}\")\n",
        "\n",
        "        # Train the final model on the entire training set\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "\n",
        "        # Predict on the test set\n",
        "        y_test_pred = model.predict(X_test_scaled)\n",
        "\n",
        "        # Calculate test accuracy\n",
        "        test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "        # Print test set accuracy\n",
        "        print(f\"Test Set Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "        # Confusion Matrix Plot\n",
        "        if ENABLE_PLOTTING:\n",
        "            conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
        "            plot_confusion_matrix(conf_matrix, np.unique(y))\n",
        "\n",
        "        # Classification Report\n",
        "        print(\"\\nClassification Report:\\n\", classification_report(y_test, y_test_pred))\n",
        "\n",
        "        # Feature Importance Plot\n",
        "        if ENABLE_PLOTTING:\n",
        "            feature_importances = model.feature_importances_\n",
        "            plot_feature_importance(feature_importances)\n",
        "\n",
        "        # Save the trained model and scaler after the final iteration\n",
        "        joblib.dump(model, '/content/drive/My Drive/urbansoundclassification/random_forest_model.pkl')\n",
        "        joblib.dump(scaler, '/content/drive/My Drive/urbansoundclassification/scaler.pkl')\n",
        "\n",
        "        # Load the saved model and scaler for testing\n",
        "        model = joblib.load('/content/drive/My Drive/urbansoundclassification/random_forest_model.pkl')\n",
        "        scaler = joblib.load('/content/drive/My Drive/urbansoundclassification/scaler.pkl')\n",
        "\n",
        "        # Process the test dataset for new predictions\n",
        "        test_audio_directory = '/content/drive/My Drive/urbansoundclassification/audio datasets1/Test'\n",
        "        test_features_df = process_audio_directory(test_audio_directory)\n",
        "\n",
        "        if test_features_df is not None:\n",
        "            # Scale new test features\n",
        "            X_new_test = test_features_df.drop(columns=['file_name'])\n",
        "            X_new_test_scaled = scaler.transform(X_new_test)\n",
        "\n",
        "            # Predict on the new test set\n",
        "            test_predictions = model.predict(X_new_test_scaled)\n",
        "\n",
        "            # Create a DataFrame for the predictions\n",
        "            output_df = pd.DataFrame({\n",
        "                'file_name': test_features_df['file_name'],\n",
        "                'predicted_class': test_predictions\n",
        "            })\n",
        "\n",
        "            # Save predictions to a CSV file\n",
        "            output_csv_path = '/content/drive/My Drive/urbansoundclassification/predictions.csv'\n",
        "            output_df.to_csv(output_csv_path, index=False)\n",
        "            print(f\"Predictions saved to '{output_csv_path}'\")\n",
        "\n",
        "            # Optional: Plot predicted class distribution\n",
        "            if ENABLE_PLOTTING:\n",
        "                plot_predicted_class_distribution(output_df)\n",
        "\n",
        "else:\n",
        "    print(\"Error: No valid features extracted from the training dataset.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQSBtbvSdHmP",
        "outputId": "c917658e-8608-4796-c7e1-9f47fe50c2b9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Directory '/content/drive/My Drive/urbansoundclassification/audio datasets2/Train' not found.\n",
            "Error: No valid features extracted from the training dataset.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}